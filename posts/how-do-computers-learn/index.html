<!doctype html><html class="dark light"><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         How Do Computers Learn?
        
    </title><meta content="How Do Computers Learn?" property=og:title><link href=/images/favicon.png rel=icon type=image/png><link href=/fonts.css rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=/atom.xml rel=alternate title=mayukh_chr type=application/atom+xml><link href=/theme/light.css rel=stylesheet><link href=/theme/dark.css id=darkModeStyle rel=stylesheet><link href=/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=/>mayukh_chr</a><div class=socials><a class=social href=https://github.com/mayukh-chr/ rel=me> <img alt=github src=/social_icons/github.svg> </a><a class=social href=https://www.linkedin.com/in/mayukh-chr/ rel=me> <img alt=linkedin src=/social_icons/linkedin.svg> </a><a href="https://open.spotify.com/user/zuumtdcue8fsfcq8owwju8i8x?si=8c3c8744363f4d95" class=social rel=me> <img alt=spotify src=/social_icons/spotify.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/projects style=margin-left:.7em>/projects</a><a href=/about style=margin-left:.7em>/about</a><a href=/cv.pdf style=margin-left:.7em>/CV</a> | <a href=javascript:void(0) id=dark-mode-toggle onclick=toggleTheme()> <img id=sun-icon src=/feather/sun.svg style=filter:invert(1)> <img id=moon-icon src=/feather/moon.svg> </a><script src=/js/themetoggle.js></script></nav></header><main><article><div class=title><div class=page-header>How Do Computers Learn?<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2024-07-29</time></div></div><h1>Table of Contents</h1><ul><li><a href=/posts/how-do-computers-learn/#introduction>Introduction</a><li><a href=/posts/how-do-computers-learn/#neurons>Neurons</a> <ul><li><a href=/posts/how-do-computers-learn/#perceptron>Perceptron</a><li><a href=/posts/how-do-computers-learn/#sigmoid-neuron>Sigmoid Neuron</a></ul><li><a href=/posts/how-do-computers-learn/#the-architecture-of-neural-networks>The architecture of neural networks</a><li><a href=/posts/how-do-computers-learn/#learning-with-gradient-descent>Learning with gradient descent</a><li><a href=/posts/how-do-computers-learn/#the-code>The code</a></ul><section class=body><h1 id=introduction>Introduction</h1><p>    At the beginning of this year, a technical club at my university hosted a "Build an AI/ML model" workshop aimed towards teaching freshers how to get started with AI. The workshop involved building a digit classification model that recognised handwritten digits using TensorFlow on Jupyter notebooks. Although it was a great beginners tutorial on TensorFlow, I felt dissatisfied with that because it didn't explain how the model actually "learnt", improving its weights and reducing the loss function.<p>    As much I'd like to go into detail about every part of the code line by line like my previous blog, it would be simply too cumbersome for my liking as a.) The code is relatively bigger, and b.) I find the mathematics behind it more fascinating. Additionally, I would highly suggest reading <a href=http://neuralnetworksanddeeplearning.com/>this textbook</a> by Michael Nielsen to have a more thorough understanding.<h1 id=neurons>Neurons</h1><p>The foundation of any neural network, whether biological or artificial, lies in its basic building blocks: neurons. In the human brain, neurons are specialized cells that transmit information through electrical and chemical signals. They form complex networks that enable us to think, learn, and perform various cognitive tasks. Similarly, in the realm of artificial intelligence, neurons are the computational units that process and transmit data within a network, enabling the system to learn and make decisions.<p>    When we talk about artificial neurons, we're referring to mathematical functions that receive one or more inputs, process them, and produce an output. These artificial neurons are designed to mimic the behavior of their biological counterparts, albeit in a much simpler and more abstract form.<h2 id=perceptron>Perceptron</h2><p>    Similar to the neurons in your brain, computer neurons re-emit signals recieved from other sources. Perhaps the simplest form of a computer neuron would be a <a href="https://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ&hl=en">perceptron</a>.<p><img alt=perceptron src=/images/p2/perceptron.png><p>A perceptron takes in one or more binary inputs (0/1) and outputs one binary value. It is calculated by<p>$$ output = \begin{cases} 0 & \text{if } \sum_j w_j x_j \leq \text{threshold} \newline 1 & \text{if } \sum_j w_j x_j > \text{threshold} \end{cases} \tag{1} $$<p>or converting $\sum_j w_j x_j$ to its dot product forms $w \cdot x$ and re-defining the threshold as the convention, bias $b$, we can rewrite it as<p>$$ output = \begin{cases} 0 & \text{if } w \cdot x - b \leq 0 \newline 1 & \text{if } w \cdot x - b > 0 \end{cases} \tag{2} $$<p>That's the basic mathematical model. It says yes or no depending on how the inputs are formed, A simple example would be: imagine your friends call you for a weekend getaway at the beach. Your decision to go depends on three factors:<ol><li>Is your boyfriend/girlfriend joining?<li>Is it going to rain?<li>Are you going by car?\end</ol><ul><li>You place option 1 as the most important, and weigh that in at $w_1 = 3$<li>You weigh in option 2 at $w_2 = 2$ and<li>$w_3 = 1$</ul><p>$x_i$ basically means "is option $i$ true or not, if it is $x_i = 1$ else its $0$. Assuming a bias $b = 5$.<p>if option 1, 3 are true and 2 is false:<p>$$w \cdot x - b = (3×1)+(2×0)+(1×1)= -1 < 0$$ so you don't go but if all 3 are true:<p>$$w\cdot x−b=(3×1)+(2×1)+(1×1)−5=3+2+1−5=1>0$$<p>So, you decide to go.<p>    This example illustrates how a perceptron can make simple decisions based on weighted inputs and a bias. Each weight represents the importance of a corresponding input, and the bias adjusts the threshold for decision-making. By adjusting the weights and bias, a perceptron can be trained to perform various logical operations and make decisions based on input data.<h2 id=sigmoid-neuron>Sigmoid Neuron</h2><p>    To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. Schematically, here's what we want (sort of):<p><img alt=learning src=/images/p2/learning.png><p>If a small change in a weight (or bias) causes a small change in the output, we can adjust the weights and biases to improve the network's performance. For example, if the network incorrectly classifies an image as an "8" instead of a "9", we can slightly change the weights and biases to make the network more likely to classify the image as a "9".<p>    Unfortunately for our perceptrons, a small change in the weights or bias of one of them in the network can sometimes switch the output of the perceptron from 1 to 0 and vice versa. That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way. So while your "9" might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-to-control way.<p>The solution to this was a new, but similar type of neuron called the sigmoid neuron. Which works with this logic:<p><img alt="sigmoid neuron" src=/images/p2/sigmoid1.png><p>It looks the same as the perceptrons that we saw earlier. Altho it has a few changes:<ol><li>Instead of the inputs being 0 or 1, these inputs can take on any value between 0 and 1, eg: 0.532... could be one input.<li>Just like a perceptron, it has weights for each input $w1, w2...$ and bias b. But the output is not 0 or 1. Its $\sigma(w \cdot x + b)$ where the <a href=https://en.wikipedia.org/wiki/Sigmoid_function>sigma function</a> (also called the logistic function) is:</ol><p>$$ \sigma(z) = \frac{1}{1 + e^{-z}} \tag{3}$$<p>    At first it looks a lot more complicated than the perceptron but suppose $z≡w⋅x+b$ is a large positive number. Then $e−z≈0$ and so $σ(z)≈1$. In other words, when $z=w⋅x+b$ is large and positive, the output from the sigmoid neuron is approximately 1, just as it would have been for a perceptron. Suppose on the other hand that $z=w⋅x+b$ is very negative. Then $e−z→∞$, and $σ(z)≈0$. So when z=w⋅x+b is very negative, the behaviour of a sigmoid neuron also closely approximates a perceptron.<p><img alt="sigmoid function graph" src=/images/p2/sigmoid-graph.png><p>So basically a sigma function is a smoother version of the step function. Which allows us to make slight changes in $w$ and $b$ for slight change in the output. As approximated by:<p>$$ \Delta \text{output} \approx \sum_j \frac{\partial \text{output}}{\partial w_j} \Delta w_j + \frac{\partial \text{output}}{\partial b} \Delta b \tag{4} $$<p>    This can be useful, for example, if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network. But sometimes it can be a nuisance. Suppose we want the output from the network to indicate either "the input image is a 9" or "the input image is not a 9". Obviously, it'd be easiest to do this if the output was a 0 or a 1, as in a perceptron. But in practice we can set up a convention to deal with this, for example, by deciding to interpret any output of at least 0.5 as indicating a "9", and any output less than 0.5 as indicating "not a 9".<h1 id=the-architecture-of-neural-networks>The architecture of neural networks</h1><p>Suppose we have the network:<p><img alt=architecture src=/images/p2/architecture2.png><p>The leftmost layer in this network is called the input layer, and the neurons within the layer are called input neurons. The rightmost or output layer contains the output neurons, or, as in this case, a single output neuron. The 2 middle layers are called hidden layers, called because the neurons in these layers are neither inputs nor outputs.<p>    Up to now, we've been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks. This means there are no loops in the network - information is always passed forward, never passed back. If we did have loops, we'd end up with situations where the input to the σ function depended on the output. That'd be hard to make sense of, and so we'll keep it outside the scope of this blog.<p>    However, There are artificial neural networks that make sense of the loops called <a href=https://aws.amazon.com/what-is/recurrent-neural-network/>recurrent neural networks</a>. The idea of which is allowing neurons to fire only a few times or a fixed duration.<p>    Our program will solve the problem of classifying individual digits and not a string of numbers. To recognize individual digits we will use a three-layer neural network:<p><img alt="alt text" src=/images/p2/architecture3.png><p>Our <a href=https://yann.lecun.com/exdb/mnist/>training data</a> for the network will consist of many $28$ by $28$ pixel images of scanned handwritten digits, and so the input layer contains $784=28×28$ neurons. The input pixels are greyscale, with a value of $0.0$ representing white, a value of $1.0$ representing black, and in between values representing gradually darkening shades of grey.<p>    The second layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by $n$, and we'll experiment with different values for $n$. The example shown illustrates a small hidden layer, containing just $n=15$ neurons.<p>    The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output $≈1$, then that will indicate that the network thinks the digit is a $0$. If the second neuron fires then that will indicate that the network thinks the digit is a $1$. And so on.<p>    A Computer Science perspective might wonder why we use 10 output neurons. After all, the goal of the network is to tell us which digit $(0,1,2,…,9)$ corresponds to the input image. A seemingly natural way of doing that is to use just $4$ output neurons, treating each neuron as taking on a binary value, depending on whether the neuron's output is closer to 0 or to 1. Four neurons are enough to encode the answer, since $2^4=16$ is more than the 10 possible values for the input digit. Why should our network use 10 neurons instead? Isn't that inefficient?<p>    This was my initial thought and turns out; one <em>could</em> implement a 4 output-neuron architecture, but the hidden layers seem to struggle with selecting features and they find 10 neurons simpler to push their outputs to. Although this is all just a heuristic, and people are free to try out different architectures just to play around.<h1 id=learning-with-gradient-descent>Learning with gradient descent</h1><p>We'll use the notation $x$ to denote a training input. It'll be convenient to regard each training input $x$ as a $28×28=784$-dimensional vector. Each vector input represents the grey value from $0$ to $1$ for each pixel. For example, if a particular training image, $x$, depicts a $6$, then $y(x)=(0,0,0,0,0,0,1,0,0,0)^T$ is the desired output from the network.<p>    In order to quantify how correct the model is we could introduce a "cost" function (also called as loss function). Similar to how your teacher gives you varying amount of marks on a answer depending on how correct it is. This function allows us to determine how correct the model is and how confident it gives its results.<p>$$C(w,b) = {\frac1 {2n}} \sum_x ||y(x) - a||^2 \tag{5}$$<p>This equation is called <a href=https://statisticsbyjim.com/regression/mean-squared-error-mse/>$mean \space squared \space error$</a> or a quadratic cost function. Here,<p>   $\bullet$ $w$ denotes the collection of all weights in the network,<p>   $\bullet$ $b$ all the biases, $n$ is the number of training inputs,<p>   $\bullet$ $a$ is the vector of outputs from the network when $x$ is input, and the sum is over<br>     all training inputs, $x$.<p>   Of course, the output a depends on $x$, $w$ and $b$, but to keep the notation simple I haven't explicitly indicated this dependence. The notation $∥v∥$ just denotes the usual length function for a vector $v$.<p>    The model has done a good job if it can find weights and biases so that $C(w,b)≈0$. By contrast, it's not doing so well when $C(w,b)$ is large - that would mean that $y(x)$ is not close to the output a for a large number of inputs. So the aim of our training algorithm will be to minimize the cost $C(w,b)$ as a function of the weights and biases. We'll minimize it using a method called gradient descent.<p>for a given cost function $C$ with parameters $v1$ and $v2$, the change in $C$ caused by changes in $v1$ and $v2$ is given by:<p>$$\Delta C \approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2}\Delta v_2$$<p>We have to choose $\Delta v_1$ and $\Delta v_2$ such that $\Delta C$ turns out to be negative as to allow for the function to descend.<p>We'll denote the gradient vector by $∇C$, i.e.:<p>$$\nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$$<p>With these definitions, the expression for $ΔC$ can be rewritten as<p>$$\Delta C \approx \nabla C \cdot \Delta v $$<p>This equation allows us to choose Δv in a way so as to make ΔC negative. In particular, suppose we choose<p>$$Δv=−η∇C$$<p>Where $η$ is a small, positive number (also called as learning rate).<p>$$ΔC≈−η∇C⋅∇C=−η∥∇C∥^2$$<p>Because $∥∇C∥2≥0$, this guarantees that $ΔC≤0$, i.e., $C$ will always decrease, never increase.<p>$$\therefore v→v'=v−η∇C \tag{6}$$<p>If we keep doing this, over and over, we'll keep decreasing $C$ until we reach (or get close to) a global minimum.<p>    You can think of this update rule as defining the gradient descent algorithm. It gives us a way of repeatedly changing the position v in order to find a minimum of the function $C$. The rule doesn't always work - several things can go wrong and prevent gradient descent from finding the global minimum of $C$. But, in practice gradient descent often works extremely well.<p>How can we apply gradient descent to learn in a neural network? The idea is to use gradient descent to find the weights $w_k$ and biases $b_l$ which minimize the cost.<p>$$w'_k = w_k - \eta \frac{\partial C}{\partial w_k}$$ $$b'_l = b_l - \eta \frac{\partial C}{\partial b_l}$$<p>    An idea called stochastic gradient descent can be used to speed up learning. The idea is to estimate the gradient $∇C$ by computing $∇C_x$ for a 'mini batch' of a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient $∇C$, and this helps speed up gradient descent, and thus learning.<p>    We pick a randomly chosen mini-batch of training inputs and train the model. We pick out another randomly chosen mini-batch and train with those. And so on, until we've exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch.<p>    We can think of stochastic gradient descent as being like political polling: it's much easier to sample a small mini-batch than it is to apply gradient descent to the full batch, just as carrying out a poll is easier than running a full election.<p>All this information until now should be enough for implementing the classification model. An implementation in python is written below with numpy.<h1 id=the-code>The code</h1><p>You can run a copy of it in your local machine (I tried it on a linux machine with 16gb of ram with no GPU). You might find some problems with importing keras.datasets, you could replace it with <code>from tensorflow.keras.datasets import mnist</code> (or even manually download it into your machine) as per your convenience.<p>    Note that this model generates ~75% accuracy (better than 10% from random guessing) and is not at all production ready, but is a very very simplified version with only sigmoid-activation functions (compared to Rectifiers and softmax activations) for educational purposes.<pre class=language-python data-lang=python style=background:#0f1419;color:#bfbab0><code class=language-python data-lang=python><span style=color:#ff7733>import </span><span>pandas </span><span style=color:#ff7733>as </span><span>pd
</span><span style=color:#ff7733>import </span><span>numpy </span><span style=color:#ff7733>as </span><span>np
</span><span style=color:#ff7733>import </span><span>pickle
</span><span style=color:#ff7733>from </span><span>keras</span><span style=color:#f29668>.</span><span>datasets </span><span style=color:#ff7733>import </span><span>mnist
</span><span style=color:#ff7733>import </span><span>matplotlib</span><span style=color:#f29668>.</span><span>pyplot </span><span style=color:#ff7733>as </span><span>plt
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>sigmoid</span><span>(</span><span style=color:#f29718>Z</span><span>):
</span><span>    </span><span style=color:#ff7733>return </span><span style=color:#f29718>1 </span><span style=color:#f29668>/ </span><span>(</span><span style=color:#f29718>1 </span><span style=color:#f29668>+ </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>exp</span><span>(</span><span style=color:#f29668>-</span><span>Z))
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>derivative_sigmoid</span><span>(</span><span style=color:#f29718>Z</span><span>):
</span><span>    sig </span><span style=color:#f29668>= </span><span style=color:#ffb454>sigmoid</span><span>(Z)
</span><span>    </span><span style=color:#ff7733>return </span><span>sig </span><span style=color:#f29668>* </span><span>(</span><span style=color:#f29718>1 </span><span style=color:#f29668>- </span><span>sig)
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>init_params</span><span>(</span><span style=color:#f29718>size</span><span>):
</span><span>    W1 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span>random</span><span style=color:#f29668>.</span><span style=color:#ffb454>rand</span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span>size) </span><span style=color:#f29668>- </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5
</span><span>    b1 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span>random</span><span style=color:#f29668>.</span><span style=color:#ffb454>rand</span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>1</span><span>) </span><span style=color:#f29668>- </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5
</span><span>    W2 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span>random</span><span style=color:#f29668>.</span><span style=color:#ffb454>rand</span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>10</span><span>) </span><span style=color:#f29668>- </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5
</span><span>    b2 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span>random</span><span style=color:#f29668>.</span><span style=color:#ffb454>rand</span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>1</span><span>) </span><span style=color:#f29668>- </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5
</span><span>    </span><span style=color:#ff7733>return </span><span>W1,b1,W2,b2
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>forward_propagation</span><span>(</span><span style=color:#f29718>X</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>W1</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>b1</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>W2</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>b2</span><span>):
</span><span>    Z1 </span><span style=color:#f29668>= </span><span>W1</span><span style=color:#f29668>.</span><span style=color:#ffb454>dot</span><span>(X) </span><span style=color:#f29668>+ </span><span>b1 
</span><span>    A1 </span><span style=color:#f29668>= </span><span style=color:#ffb454>sigmoid</span><span>(Z1) 
</span><span>    Z2 </span><span style=color:#f29668>= </span><span>W2</span><span style=color:#f29668>.</span><span style=color:#ffb454>dot</span><span>(A1) </span><span style=color:#f29668>+ </span><span>b2
</span><span>    A2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>sigmoid</span><span>(Z2)
</span><span>    </span><span style=color:#ff7733>return </span><span>Z1, A1, Z2, A2
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>mse_loss</span><span>(</span><span style=color:#f29718>A2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Y</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>m</span><span>):
</span><span>    Y_one_hot </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>identity</span><span>(A2</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>0</span><span>])[Y]</span><span style=color:#f29668>.</span><span>T 
</span><span>    </span><span style=color:#ff7733>return </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>mean</span><span>(np</span><span style=color:#f29668>.</span><span style=color:#ffb454>sum</span><span>((A2 </span><span style=color:#f29668>- </span><span>Y_one_hot)</span><span style=color:#f29668>**</span><span style=color:#f29718>2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>axis</span><span style=color:#f29668>=</span><span style=color:#f29718>0</span><span>))
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>backward_propagation</span><span>(</span><span style=color:#f29718>X</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Y</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>A1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>A2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Z1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>m</span><span>):
</span><span>    Y_one_hot </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>identity</span><span>(A2</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>0</span><span>])[Y]</span><span style=color:#f29668>.</span><span>T 
</span><span>    dZ2 </span><span style=color:#f29668>= </span><span style=color:#f29718>2 </span><span style=color:#f29668>* </span><span>(A2 </span><span style=color:#f29668>- </span><span>Y_one_hot) </span><span style=color:#f29668>/ </span><span>m
</span><span>    dW2 </span><span style=color:#f29668>= </span><span>dZ2</span><span style=color:#f29668>.</span><span style=color:#ffb454>dot</span><span>(A1</span><span style=color:#f29668>.</span><span>T)
</span><span>    db2 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>sum</span><span>(dZ2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>axis</span><span style=color:#f29668>=</span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>keepdims</span><span style=color:#f29668>=</span><span style=color:#f29718>True</span><span>)
</span><span>    dZ1 </span><span style=color:#f29668>= </span><span>W2</span><span style=color:#f29668>.</span><span>T</span><span style=color:#f29668>.</span><span style=color:#ffb454>dot</span><span>(dZ2) </span><span style=color:#f29668>* </span><span style=color:#ffb454>derivative_sigmoid</span><span>(Z1)
</span><span>    dW1 </span><span style=color:#f29668>= </span><span>dZ1</span><span style=color:#f29668>.</span><span style=color:#ffb454>dot</span><span>(X</span><span style=color:#f29668>.</span><span>T)
</span><span>    db1 </span><span style=color:#f29668>= </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>sum</span><span>(dZ1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>axis</span><span style=color:#f29668>=</span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>keepdims</span><span style=color:#f29668>=</span><span style=color:#f29718>True</span><span>)
</span><span>    </span><span style=color:#ff7733>return </span><span>dW1, db1, dW2, db2
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>update_params</span><span>(</span><span style=color:#f29718>alpha</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>b1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>b2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>dW1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>db1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>dW2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>db2</span><span>):
</span><span>    W1 </span><span style=color:#f29668>-= </span><span>alpha </span><span style=color:#f29668>* </span><span>dW1
</span><span>    b1 </span><span style=color:#f29668>-= </span><span>alpha </span><span style=color:#f29668>* </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>reshape</span><span>(db1</span><span style=color:#bfbab0cc>, </span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>1</span><span>))
</span><span>    W2 </span><span style=color:#f29668>-= </span><span>alpha </span><span style=color:#f29668>* </span><span>dW2
</span><span>    b2 </span><span style=color:#f29668>-= </span><span>alpha </span><span style=color:#f29668>* </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>reshape</span><span>(db2</span><span style=color:#bfbab0cc>, </span><span>(</span><span style=color:#f29718>10</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>1</span><span>))
</span><span>
</span><span>    </span><span style=color:#ff7733>return </span><span>W1, b1, W2, b2
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>get_predictions</span><span>(</span><span style=color:#f29718>A2</span><span>):
</span><span>    </span><span style=color:#ff7733>return </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>argmax</span><span>(A2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>0</span><span>)
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>get_accuracy</span><span>(</span><span style=color:#f29718>predictions</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Y</span><span>):
</span><span>    </span><span style=color:#ff7733>return </span><span>np</span><span style=color:#f29668>.</span><span style=color:#ffb454>sum</span><span>(predictions </span><span style=color:#f29668>== </span><span>Y)</span><span style=color:#f29668>/</span><span>Y</span><span style=color:#f29668>.</span><span>size
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>gradient_descent</span><span>(</span><span style=color:#f29718>X</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Y</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>alpha</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>iterations</span><span>):
</span><span>    size, m </span><span style=color:#f29668>= </span><span>X</span><span style=color:#f29668>.</span><span>shape
</span><span>    W1, b1, W2, b2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>init_params</span><span>(size)
</span><span>
</span><span>    </span><span style=color:#ff7733>for </span><span>i </span><span style=color:#ff7733>in </span><span style=color:#f07178>range</span><span>(iterations):
</span><span>        Z1, A1, Z2, A2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>forward_propagation</span><span>(X</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span>        dW1, db1, dW2, db2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>backward_propagation</span><span>(X</span><span style=color:#bfbab0cc>, </span><span>Y</span><span style=color:#bfbab0cc>, </span><span>A1</span><span style=color:#bfbab0cc>, </span><span>A2</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>Z1</span><span style=color:#bfbab0cc>, </span><span>m)
</span><span>        W1, b1, W2, b2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>update_params</span><span>(alpha</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2</span><span style=color:#bfbab0cc>, </span><span>dW1</span><span style=color:#bfbab0cc>, </span><span>db1</span><span style=color:#bfbab0cc>, </span><span>dW2</span><span style=color:#bfbab0cc>, </span><span>db2)
</span><span>        
</span><span>        </span><span style=color:#ff7733>if </span><span>(i</span><span style=color:#f29668>+</span><span style=color:#f29718>1</span><span>) </span><span style=color:#f29668>% </span><span style=font-style:italic;color:#39bae6>int</span><span>(iterations</span><span style=color:#f29668>/</span><span style=color:#f29718>10</span><span>) </span><span style=color:#f29668>== </span><span style=color:#f29718>0</span><span>:
</span><span>            </span><span style=color:#f07178>print</span><span>(</span><span style=color:#ff7733>f</span><span style=color:#c2d94c>"Iteration: </span><span>{i</span><span style=color:#f29668>+</span><span style=color:#f29718>1</span><span>}</span><span style=color:#c2d94c> / </span><span>{iterations}</span><span style=color:#c2d94c>"</span><span>)
</span><span>            prediction </span><span style=color:#f29668>= </span><span style=color:#ffb454>get_predictions</span><span>(A2)
</span><span>            </span><span style=color:#f07178>print</span><span>(</span><span style=color:#ff7733>f</span><span style=color:#c2d94c>'Accuracy: </span><span>{</span><span style=color:#ffb454>get_accuracy</span><span>(prediction</span><span style=color:#bfbab0cc>, </span><span>Y)</span><span style=color:#f29718>:.3%</span><span>}</span><span style=color:#c2d94c>'</span><span>)
</span><span>            </span><span style=color:#f07178>print</span><span>(</span><span style=color:#ff7733>f</span><span style=color:#c2d94c>'MSE Loss: </span><span>{</span><span style=color:#ffb454>mse_loss</span><span>(A2</span><span style=color:#bfbab0cc>, </span><span>Y</span><span style=color:#bfbab0cc>, </span><span>m)</span><span style=color:#f29718>:.4f</span><span>}</span><span style=color:#c2d94c>'</span><span>)
</span><span>    </span><span style=color:#ff7733>return </span><span>W1, b1, W2, b2
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>make_predictions</span><span>(</span><span style=color:#f29718>X</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W1 </span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>b1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>b2</span><span>):
</span><span>    </span><span style=font-style:italic;color:#39bae6>_</span><span>, </span><span style=font-style:italic;color:#39bae6>_</span><span>, </span><span style=font-style:italic;color:#39bae6>_</span><span>, A2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>forward_propagation</span><span>(X</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span>    predictions </span><span style=color:#f29668>= </span><span style=color:#ffb454>get_predictions</span><span>(A2)
</span><span>    </span><span style=color:#ff7733>return </span><span>predictions
</span><span>
</span><span style=color:#ff7733>def </span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>index</span><span style=color:#bfbab0cc>,</span><span style=color:#f29718>X</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>Y</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>b1</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>W2</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>b2</span><span>):
</span><span>   </span><span style=font-style:italic;color:#5c6773># None => creates a new axis of dimension 1, this has the effect of transposing X[:,index] which is an np.array of dimension 1 (row) and which becomes a vector (column)
</span><span>     </span><span style=font-style:italic;color:#5c6773># which corresponds to what is requested by make_predictions which expects a matrix whose columns are the pixels of the image, there we give a single column
</span><span>    vect_X </span><span style=color:#f29668>= </span><span>X[</span><span style=color:#bfbab0cc>:</span><span>, index,</span><span style=color:#f29718>None</span><span>]
</span><span>    prediction </span><span style=color:#f29668>= </span><span style=color:#ffb454>make_predictions</span><span>(vect_X</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span>    label </span><span style=color:#f29668>= </span><span>Y[index]
</span><span>    </span><span style=color:#f07178>print</span><span>(</span><span style=color:#c2d94c>"Prediction: "</span><span style=color:#bfbab0cc>, </span><span>prediction)
</span><span>    </span><span style=color:#f07178>print</span><span>(</span><span style=color:#c2d94c>"Label: "</span><span style=color:#bfbab0cc>, </span><span>label)
</span><span>
</span><span>    current_image </span><span style=color:#f29668>= </span><span>vect_X</span><span style=color:#f29668>.</span><span style=color:#ffb454>reshape</span><span>((WIDTH</span><span style=color:#bfbab0cc>, </span><span>HEIGHT)) </span><span style=color:#f29668>* </span><span>SCALE_FACTOR
</span><span>
</span><span>    plt</span><span style=color:#f29668>.</span><span style=color:#ffb454>gray</span><span>()
</span><span>    plt</span><span style=color:#f29668>.</span><span style=color:#ffb454>imshow</span><span>(current_image</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>interpolation</span><span style=color:#f29668>=</span><span style=color:#c2d94c>'nearest'</span><span>)
</span><span>    plt</span><span style=color:#f29668>.</span><span style=color:#ffb454>show</span><span>()
</span><span>
</span><span style=font-style:italic;color:#5c6773>#MAIN
</span><span>
</span><span>(X_train</span><span style=color:#bfbab0cc>, </span><span>Y_train), (X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test) </span><span style=color:#f29668>= </span><span>mnist</span><span style=color:#f29668>.</span><span style=color:#ffb454>load_data</span><span>()
</span><span>SCALE_FACTOR </span><span style=color:#f29668>= </span><span style=color:#f29718>255 </span><span style=font-style:italic;color:#5c6773># to prevent overflow on exp();
</span><span>WIDTH </span><span style=color:#f29668>= </span><span>X_train</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>1</span><span>]
</span><span>HEIGHT </span><span style=color:#f29668>= </span><span>X_train</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>2</span><span>]
</span><span>
</span><span>X_train </span><span style=color:#f29668>= </span><span>X_train</span><span style=color:#f29668>.</span><span style=color:#ffb454>reshape</span><span>(X_train</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>0</span><span>]</span><span style=color:#bfbab0cc>,</span><span>WIDTH</span><span style=color:#f29668>*</span><span>HEIGHT)</span><span style=color:#f29668>.</span><span>T </span><span style=color:#f29668>/ </span><span>SCALE_FACTOR
</span><span>X_test </span><span style=color:#f29668>= </span><span>X_test</span><span style=color:#f29668>.</span><span style=color:#ffb454>reshape</span><span>(X_test</span><span style=color:#f29668>.</span><span>shape[</span><span style=color:#f29718>0</span><span>]</span><span style=color:#bfbab0cc>,</span><span>WIDTH</span><span style=color:#f29668>*</span><span>HEIGHT)</span><span style=color:#f29668>.</span><span>T  </span><span style=color:#f29668>/ </span><span>SCALE_FACTOR
</span><span>
</span><span>W1, b1, W2, b2 </span><span style=color:#f29668>= </span><span style=color:#ffb454>gradient_descent</span><span>(X_train</span><span style=color:#bfbab0cc>, </span><span>Y_train</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>25</span><span style=color:#bfbab0cc>, </span><span style=color:#f29718>200</span><span>)
</span><span>
</span><span style=color:#ff7733>with </span><span style=color:#f07178>open</span><span>(</span><span style=color:#c2d94c>"trained_params.pkl"</span><span style=color:#bfbab0cc>,</span><span style=color:#c2d94c>"wb"</span><span>) </span><span style=color:#ff7733>as </span><span>dump_file:
</span><span>    pickle</span><span style=color:#f29668>.</span><span style=color:#ffb454>dump</span><span>((W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)</span><span style=color:#bfbab0cc>,</span><span>dump_file)
</span><span>
</span><span style=color:#ff7733>with </span><span style=color:#f07178>open</span><span>(</span><span style=color:#c2d94c>"trained_params.pkl"</span><span style=color:#bfbab0cc>,</span><span style=color:#c2d94c>"rb"</span><span>) </span><span style=color:#ff7733>as </span><span>dump_file:
</span><span>    W1, b1, W2, b2</span><span style=color:#f29668>=</span><span>pickle</span><span style=color:#f29668>.</span><span style=color:#ffb454>load</span><span>(dump_file)
</span><span>
</span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>,</span><span>X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>,</span><span>X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>2</span><span style=color:#bfbab0cc>,</span><span>X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>100</span><span style=color:#bfbab0cc>,</span><span>X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span style=color:#ffb454>show_prediction</span><span>(</span><span style=color:#f29718>200</span><span style=color:#bfbab0cc>,</span><span>X_test</span><span style=color:#bfbab0cc>, </span><span>Y_test</span><span style=color:#bfbab0cc>, </span><span>W1</span><span style=color:#bfbab0cc>, </span><span>b1</span><span style=color:#bfbab0cc>, </span><span>W2</span><span style=color:#bfbab0cc>, </span><span>b2)
</span><span>
</span></code></pre></section></article></main></div>