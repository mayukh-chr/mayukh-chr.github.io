<!doctype html><html class="dark light"><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
         Quake III's Fast Inverse Square Root Algorithm
        
    </title><meta content="Quake III's Fast Inverse Square Root Algorithm" property=og:title><link href=/images/favicon.png rel=icon type=image/png><link href=/fonts.css rel=stylesheet><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]}}</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=/atom.xml rel=alternate title=mayukh_chr type=application/atom+xml><link href=/theme/light.css rel=stylesheet><link href=/theme/dark.css id=darkModeStyle rel=stylesheet><link href=/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><a href=/>mayukh_chr</a><div class=socials><a class=social href=https://github.com/mayukh-chr/ rel=me> <img alt=github src=/social_icons/github.svg> </a><a class=social href=https://www.linkedin.com/in/mayukh-chr/ rel=me> <img alt=linkedin src=/social_icons/linkedin.svg> </a><a href="https://open.spotify.com/user/zuumtdcue8fsfcq8owwju8i8x?si=8c3c8744363f4d95" class=social rel=me> <img alt=spotify src=/social_icons/spotify.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/projects style=margin-left:.7em>/projects</a><a href=/about style=margin-left:.7em>/about</a><a href=/cv.pdf style=margin-left:.7em>/CV</a> | <a href=javascript:void(0) id=dark-mode-toggle onclick=toggleTheme()> <img id=sun-icon src=/feather/sun.svg style=filter:invert(1)> <img id=moon-icon src=/feather/moon.svg> </a><script src=/js/themetoggle.js></script></nav></header><main><article><div class=title><div class=page-header>Quake III's Fast Inverse Square Root Algorithm<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2024-01-26</time></div></div><h1>Table of Contents</h1><ul><li><a href=/posts/fast-inverse-square-root-algorithm/#introduction>Introduction</a><li><a href=/posts/fast-inverse-square-root-algorithm/#the-code>The Code</a><li><a href=/posts/fast-inverse-square-root-algorithm/#why-does-a-game-engine-need-this-function>Why does a game engine need this function?</a><li><a href=/posts/fast-inverse-square-root-algorithm/#why-a-new-algorithm>Why a new algorithm?</a><li><a href=/posts/fast-inverse-square-root-algorithm/#floating-point-numbers>Floating point numbers</a> <ul><li><a href=/posts/fast-inverse-square-root-algorithm/#tldr>TLDR</a></ul><li><a href=/posts/fast-inverse-square-root-algorithm/#bits-and-numbers>Bits and Numbers</a><li><a href=/posts/fast-inverse-square-root-algorithm/#the-steps>The Steps</a> <ul><li><a href=/posts/fast-inverse-square-root-algorithm/#iterating-through-the-code>Iterating through the code</a><li><a href=/posts/fast-inverse-square-root-algorithm/#evil-floating-point-bit-hack>Evil floating point bit hack</a><li><a href=/posts/fast-inverse-square-root-algorithm/#what-the-fuck>What the fuck?</a><li><a href=/posts/fast-inverse-square-root-algorithm/#first-iteration>First Iteration</a></ul><li><a href=/posts/fast-inverse-square-root-algorithm/#subsequent-improvements>Subsequent Improvements</a></ul><section class=body><h1 id=introduction>Introduction</h1><p>    The <strong>Fast inverse square root</strong> or <strong>0x5F3759DF</strong> is an algorithm that approximates $f(x) = 1/\sqrt x$ where $x$ is a 32-bit floating-point number. First observed in the game engine for Quake III Arena in 1999. Which was heavily based on 3D graphics.<p>In this piece of writing I will try my best on explaining the mathematical, physical and computational reason of it's existence and why it is such an ingenius hack.<p>The reason I am doing this is because this algorithm implements concepts from statistical approximation in mathematics, 3D-vectors in theoretical physics, the numerical data storing system in computer architecture, C language, and some bitwise black magic. All of which were taught to me in my 1st and 2nd years of university, just seperately.<h1 id=the-code>The Code</h1><p>For you impatient, goldfish-brained, tiktok people, here's the function of the code from Quake III Arena, stripped of the C directives (basic boilerplate), with the original comments in place.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>float </span><span style=color:#ffb454>q_rsqrt</span><span>(</span><span style=color:#ff7733>float </span><span style=color:#f29718>number</span><span>)
</span><span>{
</span><span>  </span><span style=color:#ff7733>long</span><span> i</span><span style=color:#bfbab0cc>;
</span><span>  </span><span style=color:#ff7733>float</span><span> x2</span><span style=color:#bfbab0cc>,</span><span> y</span><span style=color:#bfbab0cc>;
</span><span>  </span><span style=color:#ff7733>const float</span><span> threehalfs </span><span style=color:#f29668>= </span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5</span><span style=color:#ff7733>F</span><span style=color:#bfbab0cc>;
</span><span>
</span><span>  x2 </span><span style=color:#f29668>=</span><span> number </span><span style=color:#f29668>* </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5</span><span style=color:#ff7733>F</span><span style=color:#bfbab0cc>;
</span><span>  y  </span><span style=color:#f29668>=</span><span> number</span><span style=color:#bfbab0cc>;
</span><span>  i  </span><span style=color:#f29668>= * </span><span>( </span><span style=color:#ff7733>long </span><span style=color:#f29668>* </span><span>) </span><span style=color:#f29668>&</span><span>y</span><span style=color:#bfbab0cc>;                       </span><span style=font-style:italic;color:#5c6773>// evil floating point bit level hacking
</span><span>  i  </span><span style=color:#f29668>= </span><span style=color:#f29718>0x5f3759df </span><span style=color:#f29668>- </span><span>( i </span><span style=color:#f29668>>> </span><span style=color:#f29718>1 </span><span>)</span><span style=color:#bfbab0cc>;               </span><span style=font-style:italic;color:#5c6773>// what the fuck?
</span><span>  y  </span><span style=color:#f29668>= * </span><span>( </span><span style=color:#ff7733>float </span><span style=color:#f29668>* </span><span>) </span><span style=color:#f29668>&</span><span>i</span><span style=color:#bfbab0cc>;
</span><span>  y  </span><span style=color:#f29668>=</span><span> y </span><span style=color:#f29668>* </span><span>( threehalfs </span><span style=color:#f29668>- </span><span>( x2 </span><span style=color:#f29668>*</span><span> y </span><span style=color:#f29668>*</span><span> y ) )</span><span style=color:#bfbab0cc>;   </span><span style=font-style:italic;color:#5c6773>// 1st iteration
</span><span>  </span><span style=font-style:italic;color:#5c6773>// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed
</span><span>
</span><span>  </span><span style=color:#ff7733>return</span><span> y</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><h1 id=why-does-a-game-engine-need-this-function>Why does a game engine need this function?</h1><p>If you want to implement lighting or reflections in your game engine, it is helpful if the vectors in your calculations are <em>normalised</em> to have magnitude 1; If you don't normalise it, things can go wrong during computations.<br> <br> For normalising a 3D vector:<br> $$\hat{a} = \frac{\vec{A}}{\sqrt{x^2+y^2+z^2}}$$ or <br> $$\hat{a} = \vec{A}*\frac{1}{\sqrt{x^2+y^2+z^2}}$$ ie.<p>$$\hat{a} = \vec{A}*\frac{1}{|A|}$$<p>multiplication and addition is easy for computers. For reasons I will mention later, the square root is relatively, an extremely slow piece of computation. And division is not much better either.<p>For a game engine, this normalisation is carried out over several thousands of surfaces each second. And these slow pieces of computations break the game because it can't catch up with the real-time need of rendering these surfaces. Therefore we need a solution that can solve this problem, even by a little bit.<h1 id=why-a-new-algorithm>Why a new algorithm?</h1><p>If you are a common pleb, you would just do something like this:<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>#include </span><span style=color:#c2d94c>&LTmath.h>
</span><span>
</span><span style=color:#ff7733>float </span><span style=color:#ffb454>q_rsqrt</span><span>(</span><span style=color:#ff7733>float </span><span style=color:#f29718>number</span><span>)
</span><span>{
</span><span>    </span><span style=color:#ff7733>return </span><span style=color:#f29718>1</span><span style=color:#f29668>/</span><span style=color:#f07178>sqrt</span><span>(x)</span><span style=color:#bfbab0cc>;
</span><span>}
</span></code></pre><p>The ALU (Arithmetic and logic unit) in our CPUs have specialized system of gates built for addition and multiplication, but not for division and subtraction. Although subtraction has a quick fix with <a href=https://www.geeksforgeeks.org/subtraction-of-two-numbers-using-2s-complement/>2's complement method</a>, Division is still a more complex calculation, even more complex is square root. Intel and other manufacturers could, in theory add specialized gates for them, but it is very expensive. For thousands of calculations a second(like in our case), these gates are simply not feasable. So we need something that can be faster. The fast inverse square root is an approximation with an error of atmost 1%, while being about 3x as fast.<h1 id=floating-point-numbers>Floating point numbers</h1><p><p>Before we proceed any further, I think it is required to mention how computers store floats. This is going to be very boring, so there's a TLDR after this.<p>Floats are expressed in a similar fashion to scientific notations; namely the <a href=https://en.wikipedia.org/wiki/IEEE_754>IEEE-754 standard</a>.<p>The smart folks at IEEE set it in this form<p><img alt=images src=/images/p1/Untitled-1.svg><ul><li><p>The first bit is a sign bit, which is 1 when the number is negative and 0 when positive. Since we'll be dealing exclusively with positive numbers with this algorithm (we would never need to calculate $\frac{1}{\sqrt{-5}}$ or something) the sign bit will always be 0.  </p><li><p>It's followed by 8 bits of exponents in Excess-127 format. For context, you can use 8 bits represent exponents from 0 to 255; However, we need to represent fractions (ie, negative exponents) too, so we shift the exponents by 127. Now we can represent exponents from -127 to 128; so the power 4, instead of 00000100, will be represented as 10000011. This shifting is called Excess-127 (because we're shifting exponents by 127).  </p><li><p>The remaining 23 bits store the fractional part, from 1.0000000.... to 1.11111.... (so $[1, 2)$ in binary). But, the IEEE realized that the first digit in scientific notation will always be 1, so they made it part of the equation. Therefore, the 23 bits now store only the Mantissa (The part after the decimal point).</p></ul><p>Note: What we've discussed just now is actually a subset of the IEEE-754 standard, which describes normalised numbers; denormalised numbers, NaN (not a number), 0 and -0 will never be accepted by our algorithm, so we won't discuss them here.<h2 id=tldr>TLDR</h2><p>The number that we recieve will have 32 bits, first one being 0, next 8 being the exponent(E) and the remaining 23 being the Mantissa(M).<p>written in the form of ${2^{23}}*E + M$.<p>$M$ = 01001110010000000000000<p>$E$ = 10001001<p>${2^{23}}*E$ = 10001001 00000000000000000000000<p>${2^{23}}*E + M$ = 10001001 01001110010000000000000<h1 id=bits-and-numbers>Bits and Numbers</h1><p>From our Floating point thing, our number can be represented as:<p>$$n = \left(1+ \frac{M}{2^{23}}\right)*2^{E-127}$$<p>Taking $log_2$ on both sides:<p>$$log_2n = log_2\left(\left(1+ \frac{M}{2^{23}}\right)*2^{E-127}\right)$$<p>Simplifying:<p>$$log_2n = log_2\left(1+ \frac{M}{2^{23}}\right)+2^{E-127}$$<p>Using $log_2(1+x) \approx x$ (<a href=https://math.stackexchange.com/a/1111064>source</a>):<p>$$log_2n = \frac{M}{2^{23}}+E+\mu-127$$<p>Note: the $\mu$ value is 0.0430, this value shifts the approximation to reduce the average error when $0 \leq x \leq 1$.<p>Multiplying and dividing by $2^{23}$<p>$$log_2n = \frac{1}{2^{23}}\left(M+2^{23}*E\right)+\mu-127$$<p>Now we see that we just got the bit representation of our number. Therefore, the log of our number stores the bit representiaton, abeit with some scaling and shifting.<h1 id=the-steps>The Steps</h1><h2 id=iterating-through-the-code>Iterating through the code</h2><p>Looking at the code again, the first 4 lines don't seem that harmful.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>float </span><span style=color:#ffb454>q_rsqrt</span><span>(</span><span style=color:#ff7733>float </span><span style=color:#f29718>number</span><span>){         </span><span style=color:#f29668>> </span><span style=color:#f29718>32</span><span> bit decimal </span><span style=color:#ffb454>number </span><span>(input)
</span><span>  </span><span style=color:#ff7733>long</span><span> i</span><span style=color:#bfbab0cc>;                            </span><span style=color:#f29668>> </span><span style=color:#f29718>32</span><span style=color:#f29668>-</span><span>bit integer number
</span><span>  </span><span style=color:#ff7733>float</span><span> x2</span><span style=color:#bfbab0cc>,</span><span> y</span><span style=color:#bfbab0cc>;                       </span><span style=color:#f29668>> </span><span style=color:#f29718>32</span><span style=color:#f29668>-</span><span>bit decimal numbers
</span><span>  </span><span style=color:#ff7733>const float</span><span> threehalfs </span><span style=color:#f29668>= </span><span style=color:#f29718>1</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5</span><span style=color:#ff7733>F</span><span style=color:#bfbab0cc>;     </span><span style=color:#f29668>> </span><span style=color:#f29718>3</span><span style=color:#f29668>/</span><span style=color:#f29718>2</span><span style=color:#bfbab0cc>,</span><span> also </span><span style=color:#f29718>32</span><span style=color:#f29668>-</span><span>bit</span><span style=color:#f29668>.
</span></code></pre><p>The next two aren't that bad either:<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>  x2 </span><span style=color:#f29668>=</span><span> number </span><span style=color:#f29668>* </span><span style=color:#f29718>0</span><span style=color:#bfbab0cc>.</span><span style=color:#f29718>5</span><span style=color:#ff7733>F</span><span style=color:#bfbab0cc>;                </span><span style=color:#f29668>></span><span> Assign number</span><span style=color:#f29668>/</span><span style=color:#f29718>2</span><span> to x2
</span><span>  y  </span><span style=color:#f29668>=</span><span> number</span><span style=color:#bfbab0cc>;                       </span><span style=color:#f29668>></span><span> Assign number to y
</span></code></pre><p>But then, all hell seems to break loose, what is <em>i</em>? What is 0x5f3759df? Why declare a variable for 1.5 and not for 0x5f3759df? Why are there so many pointers?<p>The comments don't seem to help either. But it hints that there's three steps in the process, namely:<ul><li>evil floating point bit hack<li>what the fuck<li>1st iteration (spoiler: Newton's approximations)</ul><h2 id=evil-floating-point-bit-hack>Evil floating point bit hack</h2><pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>i </span><span style=color:#f29668>= *</span><span>( </span><span style=color:#ff7733>long </span><span style=color:#f29668>* </span><span>) </span><span style=color:#f29668>&</span><span>y</span><span style=color:#bfbab0cc>;
</span></code></pre><p>Now the problem with floats in C is that it doesn't support <a href=https://en.wikipedia.org/wiki/Bit_manipulation>bit manipulations</a>.<p>But longs do.<p>So this line hacks C into thinking that the content stored in <code>y</code>'s address is actually a long, and then stores the data into i, which is actually a long.<p><code>&y</code> $\Rightarrow$ get the memory address of y.<br> <code>(long * )&y</code> $\Rightarrow$ turn the data stored in the address y into a long.<br> <code>i = *( long * ) &y;</code> $\Rightarrow$ store that data in i.<p>Now we know that both floats and longs have 32 bit memory allocations, So this line creates a one-to-one mapping of the bits from <code>y</code> to <code>i</code>. Now our number can go through bit manipulations.<h2 id=what-the-fuck>What the fuck?</h2><pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>i  </span><span style=color:#f29668>= </span><span style=color:#f29718>0x5f3759df </span><span style=color:#f29668>- </span><span>( i </span><span style=color:#f29668>>> </span><span style=color:#f29718>1 </span><span>)</span><span style=color:#bfbab0cc>; 
</span></code></pre><p>Let's talk about bit manipulation, namely shifting. In binary, shifting left doubles the number and shifting right halves it, while rounding it off.<ul><li>Let: x = 1101 = 13<li>(x << 1) = 11010 = 26<li>(x >> 1) = 110 = 6</ul><p>Lets see how bit shifting affects exponents:<ul><li>let our exponent be $n^x$.<li>left shifting a exponent doubles it. ie: $n^{2x}$<li>right shifting a exponent gives us the square root. ie: $n^{\frac{x}{2}}$</ul><p>We have our number $y$ and we have to find out $\frac{1}{\sqrt{y}}$<p>But as we've seen:<p>$$log_2(y) \approx i$$<p>so let's just calculate<p>$$log_2\left(\frac{1}{\sqrt{y}}\right)$$<p>which is equal to:<p>$$-\frac{1}{2}log_2(y)$$<p>Calculating this is stupidly easy. "But you just told me that division is difficult!!1!!" Yes but remember bit shifting???<br> Just do <code>-(i >> 1)</code> and you're all set.<p>Now you might be wondering how and why do we have <code>0x5f3759df</code> in the line. Go to the end of <a href=https://mayukh-chr.github.io/posts/fast-inverse-square-root-algorithm/#bits-and-numbers>this</a> and read "abeit with some scaling and shifting.". Meaning that we need to scale and shift it back by some constant.<p>Let $$log(\Gamma) = log\left(\frac{1}{\sqrt{y}}\right)$$<p>which equals to<p>$$log(\Gamma) = -\frac{1}{2}log_2(y)$$<p>Now we replace the logarithm with the bit representation<p>$$\frac{1}{2^{23}}\left(M_\Gamma + 2^{23}*E_\Gamma\right) + \mu -127 = -\frac{1}{2}\left(\frac{1}{2^{23}}(M_\Gamma + 2^{23}*E_\Gamma) + \mu -127\right)$$<p>Calculating for $\left(M_\Gamma + 2^{23}*E_\Gamma\right)$<p>$$\left(M_\Gamma + 2^{23}*E_\Gamma\right) = \frac{3}{2}2^{23}\left(127-\mu\right)- \frac{1}{2} \left(M_y + 2^{23}*E_y\right)$$<p>where: $\frac{3}{2}2^{23}\left(127-0.0430\right) \approx$ 0x5F3759DF.<p>Therefore:<p>= 0x5F3759DF - (i >> 1) (Note: Much later a more accurate constant was derived: 0x5F375A86, but we'll ignore that for now.)<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>y  </span><span style=color:#f29668>= * </span><span>( </span><span style=color:#ff7733>float </span><span style=color:#f29668>* </span><span>) </span><span style=color:#f29668>&</span><span>i</span><span style=color:#bfbab0cc>;
</span></code></pre><p>This is just reversing the steps of the evil bit hack to get back the actual approximation of those bits.<h2 id=first-iteration>First Iteration</h2><pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span>y  </span><span style=color:#f29668>=</span><span> y </span><span style=color:#f29668>* </span><span>( threehalfs </span><span style=color:#f29668>- </span><span>( x2 </span><span style=color:#f29668>*</span><span> y </span><span style=color:#f29668>*</span><span> y ) )</span><span style=color:#bfbab0cc>;
</span></code></pre><p>After the previous step, we have a pretty good approximation but we did pick up some error terms here and there, but we can use <a href=https://en.wikipedia.org/wiki/Newton%27s_method>Newton's approximation</a> to get a close result.<p>Newton's method finds a root of an equation. ie it finds an $x$ for which $f(x) = 0$. You repeat this process until you're satisfied with your solution. But in this case, our initial approximation is good enough that one iteration of it gets our error to $\leq1%$.<p>The only thing that newton's method needs is a function and it's derivative. To put it simply:<p>$$x_{new} = x - \frac{f(x)}{f\prime(x)}$$<p>here it's:<p>$$f(y) = 0 = \frac{1}{y^2}-x$$<p>$$\because f(y) = 0$$ $$\therefore y = \frac{1}{\sqrt{x}}$$<p>therefore using newton's method:<p>$$f(y) = \frac{1}{y^2}-x \space and \space f\prime(y) = -\frac{2}{y^3}$$<p>$$y_{n+1} = y_n - \frac{f(y_n)}{f\prime(y_n)}$$ which is equal to $$y_{n+1} = \frac{y_n\left(3-xy_n^2\right)}{2} $$<p>which is the last line.<pre class=language-c data-lang=c style=background:#0f1419;color:#bfbab0><code class=language-c data-lang=c><span style=color:#ff7733>return</span><span> y</span><span style=color:#bfbab0cc>;
</span><span style=color:#ff3333>}
</span></code></pre><p>just return the result that we just got. and you're done.<h1 id=subsequent-improvements>Subsequent Improvements</h1><p>It is not known precisely how the exact value for the magic number was determined. But many speculate it's been through trial and error. Chris Lomont developed a function to minimize approximation error by choosing the magic number R R over a range. He first computed the optimal constant for the linear approximation step as 0x5F37642F, close to 0x5F3759DF, but this new constant gave slightly less accuracy after one iteration of Newton's method. Lomont then searched for a constant optimal even after one and two Newton iterations and found 0x5F375A86, which is more accurate than the original at every iteration stage.<p>Subsequent additions by hardware manufacturers have made this algorithm redundant for the most part. For example, on x86, Intel introduced the SSE instruction rsqrtss in 1999. In a 2009 benchmark on the Intel Core 2, this instruction took 0.85ns per float compared to 3.54ns for the fast inverse square root algorithm, and had less error.</section></article></main></div>